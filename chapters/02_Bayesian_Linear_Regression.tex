\section{Bayesian Linear Regression}
\(\vy=\mX\vw+\vepsilon, \vepsilon\sim\N{\vmu}{\sigma_n^2 \mI_d}\)
\begin{framed}
    \textbf{Solutions}:
    $\vwhat_{\ls} = \inv{(\transpose{\mX} \mX)} \transpose{\mX} \vy$  \\
    $\vwhat_{\ridge} = \inv{(\transpose{\mX} \mX + \lambda \mI)} \transpose{\mX} \vy$ where \(\lambda=\sigma_n^2/\sigma_p^2\)
\end{framed}
\textbf{Notable Results}: $\Var{\vwhat_{\ls} \mid \rX} = \sigman^2(\transpose{\rX}\rX)^{-1}$ \\
\textbf{Gaussian prior} $\vw \sim \N{\vzero}{\sigmap^2 \mI}$ yields posterior $\log p(\vw \mid \vx_{1:n}, y_{1:n}) = -\frac{1}{2} \brackets*{\transpose{\vw}\mSigma^{-1}\vw - 2\vmu} + \const$, $\mSigma \defeq \inv{\parentheses*{\sigman^{-2} \transpose{\mX} \mX + \sigmap^{-2} \mI}}$, $\vmu \defeq \sigman^{-2} \mSigma \transpose{\mX} \vy$. We have $\vw \mid \vx_{1:n}, y_{1:n} \sim \N{\vmu}{\mSigma}$: \textit{Gaussian's with known variance and linear likelihood} are self-conjugate. \\
$\vwhat_\MAP = \argmin_\vw \norm{\vy - \mX \vw}_2^2 + \frac{\sigman^2}{\sigmap^2} \norm{\vw}_2^2$, \textit{identical to ridge regression} with $\lambda \defeq \sigman^2 / \sigmap^2$. \\
A \textbf{Laplace prior} on the weights is equivalent to \textbf{lasso regression} with decay ${\lambda \defeq \sigman^2 / \ell}$. \\
\textbf{Bayesian inference}: For test point $\vxs$, $\ys\mid  y_{1:n} \sim \N{\transpose{\vmu} \vxs}{\transpose{\vxs} \mSigma \vxs + \sigman^2}$. \\
$\Var{\ys} = \underbrace{\E[\vtheta]{\Var[\ys]{\ys, \vtheta}}}_{\text{aleatoric uncertainty}} + \underbrace{\Var[\vtheta]{\E[\ys]{\ys, \vtheta}}}_{\text{epistemic uncertainty}}$.\\
Aleatoric $\rightarrow$ noise in data; Epistemic $\rightarrow$ noise in model.
Online inference requires \(\mathcal{O}(d)\) memory and \(\mathcal{O}(d^2)\) time per-round.
Applying linear regression to non linear functions: apply a non linear transformation $\vphi$ to $\rX$. Define $\mPhi = \vphi(\rX)$, so-called \textbf{Kernel}. With a gaussian prior we get: $\vf \mid \mX \sim \N{\mPhi \E{\vw}}{\mPhi \Var{\vw} \transpose{\mPhi}} = \N{\vzero}{\mK}$, with $\mK = \sigmap^2 \mPhi \transpose{\mPhi}$. We define the \textbf{Kernel-function}: $k(\vx, \vxp) \defeq \sigmap^2 \cdot \transpose{\vphi(\vx)} \vphi(\vxp) = \Cov{f(\vx), f(\vxp)}$.
\begin{framed}
    \textbf{Linear}: $k(\vx, \vxp) = l \transpose{\vx} \vxp$ or \(l\phi(\vx)^T \phi(\vx')\)\\
    \textbf{RBF/Gaussian}: $k(\vx, \vxp) = \exp{-\frac{(\vx - \vxp)^2}{2\sigma_p^2}}$ \\
    \textbf{Polynomial} $k(\vx, \vxp) = (1 + \transpose{\vx} \vxp)^d$ \\
    \textbf{Laplacian}: $k(\vx, \vxp) = \exp{\left(-\alpha \norm{\vx - \vxp}\right)}$
\end{framed}
\begin{framed}
    \textbf{Properties of Kernels}:  \\
    Symmetry: $k(\vx, \vxp) = k(\vxp, \vx)$ and $\mK_{\sA\sA}$ is p.s.d. \\
    Kernels can be \textbf{composed} in the following ways: addition, multiplication, positive scalar multiplication and composition with a funtion $f$ if $f$ is polynomial with positive coefficients or $\exp$.
\end{framed}
\begin{framed}
    \textbf{Positive Semidefiniteness}
    If kernel matrix \(\mK\) is p.s.d., then  \(\forall \vx\neq 0\), \(\vx^T \mK \vx >0\).
    If \(\det{\mK}>0\),  \(\mK\) is p.s.d. If \(\det{\mK}<0\),  \(\mK\) is not p.s.d. No result for \(\det{\mK}=0\).

    
\end{framed}
\textbf{Stationary} if there exists a $\tilde{k}$ s.t. $\tilde{k}(\vx - \vxp) = k(\vx, \vxp)$, and \textbf{Isotropic} if there exists a $\tilde{k}$ s.t. $\tilde{k}(\norm{\vx-\vxp}_2) = k(\vx, \vxp)$.

